# -*- coding: utf-8 -*-
"""9_Sklearn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14jLbC65CPj-JVvhmt-xOwuKiyAMogW0o

# Central Limit Theorem (CLT)
---

The Central Limit Theorem (CLT) states that the distribution of the sample mean approaches a normal distribution, regardless of the population's original distribution, as long as the sample size is sufficiently large
"""

# Data processing
# ==============================================================================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Warnings
# ==============================================================================
import warnings
warnings.filterwarnings('ignore')

"""# Introduction to Cross Validation

In this lecture series we will do a much deeper dive into various methods of cross-validation. As well as a discussion on the general philosphy behind cross validation. A nice official documentation guide can be found here: https://scikit-learn.org/stable/modules/cross_validation.html

----
----
----
## Cross Validation with cross_val_score

----

<img src="https://editor.analyticsvidhya.com/uploads/47515k-fold.jpg">

----

**K-Fold Cross-Validation**

In this resampling technique, the whole data is divided into k sets of almost equal sizes. The first set is selected as the test set and the model is trained on the remaining k-1 sets. The test error rate is then calculated after fitting the model to the test data.

In the second iteration, the 2nd set is selected as a test set and the remaining k-1 sets are used to train the data and the error is calculated. This process continues for all the k sets.
"""

# Import necessary libraries

from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.metrics import accuracy_score

# Load a simple dataset (Iris)
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = pd.Series(iris.target)

X.head()

X.info()

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=102)
X_train

# Initialize the Logistic Regression model
model = LogisticRegression(max_iter=100)

# --- Part 1: Train-Test Split ---

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=102)

# Train the model on the training set
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate the accuracy of the model on the test set
test_train_split_accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy with Train-Test Split: {test_train_split_accuracy:.4f}")

# --- Part 2: K-Fold Cross-Validation ---

# Define the K-Fold cross-validator (5 folds)
kf = KFold(n_splits=5, shuffle=True, random_state=102)

# Perform K-Fold Cross-Validation and compute accuracy for each fold
cv_scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')

# Print accuracy scores for each fold
print(f"Accuracy scores for each fold (K-Fold): {cv_scores}")

# Calculate mean and standard deviation of the K-Fold accuracy scores
mean_cv_accuracy = np.mean(cv_scores)
std_cv_accuracy = np.std(cv_scores)

print(f"Mean accuracy with K-Fold CV: {mean_cv_accuracy:.4f}")
print(f"Standard deviation of accuracy with K-Fold CV: {std_cv_accuracy:.4f}")

# --- Comparison ---
print("\n--- Comparison ---")
print(f"Accuracy with Train-Test Split: {test_train_split_accuracy:.4f}")
print(f"Mean accuracy with K-Fold CV: {mean_cv_accuracy:.4f}")
print(f"Standard deviation of K-Fold CV accuracy: {std_cv_accuracy:.4f}")

"""# Explanation of results:
- The cross_val_score runs the model on different train-test splits using K-Fold.
- The average accuracy provides an estimate of the model's performance on unseen data.
- The standard deviation shows how much the accuracy varies across different folds.

# Hyperparameter Tuning
"""

df = pd.read_csv("AMES_Final_DF.csv")

df.tail()

"""**The label we are trying to predict is the SalePrice column. Separate out the data into X features and y labels**"""

X = df.drop('SalePrice',axis=1)
y = df['SalePrice']

"""**Use scikit-learn to split up X and y into a training set and test set. Since we will later be using a Grid Search strategy, set your test proportion to 10%. To get the same data split as the solutions notebook, you can specify random_state = 101**"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=101)

"""**The dataset features has a variety of scales and units. For optimal regression performance, scale the X features. Take carefuly note of what to use for .fit() vs what to use for .transform()**"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

scaled_X_train = scaler.fit_transform(X_train)
scaled_X_test = scaler.transform(X_test)

"""**We will use an Elastic Net model. Create an instance of default ElasticNet model with scikit-learn**"""

from sklearn.linear_model import ElasticNet


base_elastic_model = ElasticNet()

"""## Elastic Net

Elastic Net is a linear regression model that combines L1 (Lasso) and L2 (Ridge) regularization. The parameters in your `param_grid` are essential for tuning the model. Let's break down what each parameter means:

### 1. `alpha`

`alpha` is the regularization strength parameter in Elastic Net. It controls the amount of penalty applied to the regression coefficients. Higher values of `alpha` increase the amount of regularization.

**Effect:**

- When `alpha` is set to `0`, the Elastic Net model behaves like a regular linear regression model (no regularization).
- As `alpha` increases, the model becomes more regularized, which can help prevent overfitting by shrinking the coefficients towards zero.
- A very high `alpha` can lead to a model where most coefficients are pushed to zero, potentially resulting in underfitting.

### 2. `l1_ratio`

The `l1_ratio` parameter determines the mix of L1 and L2 regularization in the Elastic Net. It takes values between `0` and `1`.

- If `l1_ratio = 0`, the model behaves like Ridge regression (only L2 regularization).
- If `l1_ratio = 1`, the model behaves like Lasso regression (only L1 regularization).
- Values between `0` and `1` mix both regularization techniques, allowing the model to leverage the strengths of both methods.

**Effect:**

- A higher `l1_ratio` (closer to `1`) emphasizes Lasso regularization, which can lead to sparse solutions where many coefficients are exactly zero, effectively performing variable selection.
- A lower `l1_ratio` (closer to `0`) emphasizes Ridge regularization, which shrinks coefficients but does not lead to sparsity.

**The Elastic Net model has two main parameters, alpha and the L1 ratio. Create a dictionary parameter grid of values for the ElasticNet. Feel free to play around with these values, keep in mind, you may not match up exactly with the solution choices**
"""

param_grid = {'alpha':[0.1,1,5,10,50,100],
              'l1_ratio':[.1, .5, .7, .9, .95, .99, 1]}

"""**Using scikit-learn create a GridSearchCV object and run a grid search for the best parameters for your model based on your scaled training data. [In case you are curious about the warnings you may recieve for certain parameter combinations](https://stackoverflow.com/questions/20681864/lasso-on-sklearn-does-not-converge)**"""

from sklearn.model_selection import GridSearchCV

# verbose number a personal preference
grid_model = GridSearchCV(estimator=base_elastic_model,
                          param_grid=param_grid,
                          scoring='neg_mean_squared_error',
                          cv=5,
                          verbose=1)

grid_model.fit(scaled_X_train,y_train)

"""**Display the best combination of parameters for your model**"""

grid_model.best_params_

"""**Evaluate your model's performance on the unseen 10% scaled test set.**"""

y_pred = grid_model.predict(scaled_X_test)

from sklearn.metrics import mean_absolute_error,mean_squared_error

mean_absolute_error(y_test,y_pred)

np.sqrt(mean_squared_error(y_test,y_pred))

np.mean(df['SalePrice'])

"""# Principal Component Analysis

### Principal Component Analysis (PCA)

**Principal Component Analysis (PCA)** is a technique used for dimensionality reduction while retaining as much information (variance) as possible from the original dataset. PCA transforms the data into a new coordinate system where the greatest variance lies along the first axis (the first principal component), and subsequent axes capture decreasing amounts of variance. This is useful for simplifying, visualizing, and analyzing high-dimensional data.

#### Key Concepts:

1. **Dimensionality Reduction**:
   - PCA reduces the number of features (dimensions) in the dataset, simplifying the data while keeping the most important information (variance).
   
2. **Principal Components**:
   - These are the new features formed by PCA. The first principal component captures the largest variance, the second captures the second-largest variance, and so on. These components are uncorrelated with each other.

3. **Variance**:
   - PCA focuses on capturing the directions of maximum variance in the data. Variance represents how spread out the data is, and the principal components with the largest variance contain the most information.

4. **Eigenvalues and Eigenvectors**:
   - PCA uses **eigenvalues** and **eigenvectors** from the covariance matrix of the data. Eigenvectors give the directions of maximum variance (principal components), while eigenvalues tell us how much variance is in those directions.

5. **Linear Transformation**:
   - PCA transforms the original data into a new coordinate system based on principal components.

#### Steps in PCA:

1. **Standardize the Data**:
   - Since PCA is affected by the scale of variables, the data is standardized (mean = 0, standard deviation = 1) before applying PCA.
   
2. **Compute Covariance Matrix**:
   - The covariance matrix captures relationships between features. It helps find directions in which the variance of the data is maximized.

3. **Calculate Eigenvalues and Eigenvectors**:
   - Eigenvectors define the directions of principal components, while eigenvalues represent the amount of variance in each direction.

4. **Sort Eigenvalues and Select Principal Components**:
   - Sort eigenvalues in descending order and select the top **k** eigenvectors (principal components) based on how much variance you want to retain.

5. **Transform the Data**:
   - Project the data onto the selected principal components to reduce its dimensionality.

#### When to Use PCA:

1. **Dimensionality Reduction**:
   - Use PCA to reduce the number of features, especially when features are correlated, or there is redundant information.

2. **Noise Reduction**:
   - PCA can eliminate noise by focusing on components that capture the most important patterns.

3. **Data Visualization**:
   - PCA is useful for visualizing high-dimensional data in 2D or 3D using the top principal components.

#### Limitations:

- **Linear Assumption**: PCA assumes linear relationships between features, so it may not capture non-linear patterns in data.
- **Interpretability**: Principal components are linear combinations of original features, which can be hard to interpret.
- **Variance Focus**: PCA focuses on variance, which might not always align with the most important features for your machine learning task.

#### Summary:
PCA simplifies high-dimensional datasets, reduces computational complexity, and helps in exploratory data analysis, preprocessing, and visualization tasks by retaining the most important information (variance).
"""

from sklearn.datasets import load_breast_cancer


# Load the breast cancer dataset
data = load_breast_cancer()

data.keys()

data.target

data.target_names

# Convert to a DataFrame for easier exploration
df = pd.DataFrame(data.data, columns=data.feature_names)
df['target'] = data.target

# Display the first few rows of the dataset
df.head()

print("\nDataset Description:\n", data.DESCR)

sns.pairplot(df[['mean radius', 'mean texture', 'mean area', 'mean smoothness']])
plt.show()

df.info()

"""## PCA with Scikit-Learn


### Scaling Data
"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

scaled_X = scaler.fit_transform(df)

scaled_X

from sklearn.decomposition import PCA

help(PCA)

pca = PCA(n_components=30)

pca = PCA(n_components=2)

principal_components = pca.fit_transform(scaled_X)

plt.figure(figsize=(8,6))
plt.scatter(principal_components[:,0],principal_components[:,1])
plt.xlabel('First principal component')
plt.ylabel('Second Principal Component')

plt.figure(figsize=(8,6))
plt.scatter(principal_components[:,0],principal_components[:,1],c=df['target'])
plt.xlabel('First principal component')
plt.ylabel('Second Principal Component')

pca = PCA()
X_pca = pca.fit_transform(scaled_X)
print(pca.n_components_)

import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(10,6))
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Explained Variance by PCA Components')
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# cumulative explained variance
cumulative_variance = np.cumsum(pca.explained_variance_ratio_)

plt.figure(figsize=(10,6))
colors = plt.cm.plasma(np.linspace(0,1,len(cumulative_variance)))  # colorful gradient

# plot colorful line + scatter
for i in range(len(cumulative_variance)-1):
    plt.plot([i, i+1], [cumulative_variance[i], cumulative_variance[i+1]], color=colors[i], lw=2)
plt.scatter(range(1, len(cumulative_variance)+1), cumulative_variance, c=colors, s=80, edgecolor='k')

plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Explained Variance by PCA Components (Colorful)')
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

print(pca.components_)

"""In this numpy matrix array, each row represents a principal component, Principal axes in feature space, representing the directions of maximum variance in the data. The components are sorted by explained_variance_.

We can visualize this relationship with a heatmap:
"""

df_comp = pd.DataFrame(pca.components_,index=['PC1','PC2'],columns=df.columns)

df_comp

plt.figure(figsize=(20,3),dpi=150)
sns.heatmap(df_comp,annot=True)

pca.explained_variance_ratio_

np.sum(pca.explained_variance_ratio_)

pca_30 = PCA(n_components=30)
pca_30.fit(scaled_X)

pca_30.explained_variance_ratio_

np.sum(pca_30.explained_variance_ratio_)

explained_variance = []

for n in range(1,30):
    pca = PCA(n_components=n)
    pca.fit(scaled_X)

    explained_variance.append(np.sum(pca.explained_variance_ratio_))

plt.plot(range(1,30),explained_variance)
plt.xlabel("Number of Components")
plt.ylabel("Variance Explained");

